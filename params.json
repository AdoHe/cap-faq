{"name":"The CAP FAQ","tagline":"","body":"# The CAP FAQ\r\n\r\n    Version 1.0, May 9th 2013\r\n    By: Henry Robinson / henry.robinson@gmail.com / @henryr\r\n<pre><a href=\"http:://the-paper-trail.org/\">http://the-paper-trail.org/</a></pre>\r\n\r\n## 0. What is this document?\r\n\r\nNo subject appears to be more controversial to distributed systems\r\nengineers than the oft-quoted, oft-misunderstood CAP theorem. The\r\npurpose of this FAQ is to explain what is known about CAP, so as to\r\nhelp those new to the theorem get up to speed quickly, and to settle\r\nsome common misconceptions or points of disagreement.\r\n\r\nOf course, there's every possibility I've made superficial or\r\ncompletely thorough mistakes here. Corrections and comments are\r\nwelcome: <a href=\"mailto:henry.robinson+cap@gmail.com\">let me have\r\nthem</a>.\r\n\r\nThere are some questions I still intend to answer. For example\r\n\r\n* *What's the relationship between CAP and performance?*\r\n* *What does CAP mean to me as an engineer?*\r\n* *What's the relationship between CAP and ACID?*\r\n\r\nPlease suggest more.\r\n\r\n## 1. Where did the CAP Theorem come from?\r\n\r\nDr. Eric Brewer gave a keynote speech at the Principles of Distributed\r\nComputing conference in 2000 called 'Towards Robust Distributed\r\nSystems' [1]. In it he posed his 'CAP Theorem' - at the time unproven - which\r\nillustrated the tensions between being correct and being always\r\navailable in distributed systems.\r\n\r\nTwo years later, Seth Gilbert and Professor Nancy Lynch - researchers\r\nin distributed systems at MIT - formalised and proved the conjecture\r\nin their paper “Brewer's conjecture and the feasibility of consistent,\r\navailable, partition-tolerant web services” [2].\r\n\r\n[1] http://www.cs.berkeley.edu/~brewer/cs262b-2004/PODC-keynote.pdf\r\n[2] http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf\r\n\r\n## 2. What does the CAP Theorem actually say?\r\n\r\nThe CAP Theorem (henceforth 'CAP') says that it is impossible to build\r\nan implementation of read-write storage in an asynchronous network that\r\nsatisfies all of the following three properties:\r\n\r\n* Availability - will a request made to the data store always eventually complete?\r\n* Consistency - will all executions of reads and writes seen by all nodes be _atomic_ or _linearizably_ consistent?\r\n* Partition tolerance - the network is allowed to drop any messages.\r\n\r\nThe next few items define any unfamiliar terms.\r\n\r\nMore informally, the CAP theorem tells us that we can't build a\r\ndatabase that both responds to every request and returns the results\r\nthat you would expect every time. It's an _impossibility_ result - it\r\ntells us that something we might want to do is actually provably out\r\nof reach. It's important now because it is directly applicable to the\r\nmany, many distributed systems which have been and are being built in\r\nthe last few years, but it is not a death knell: it does _not_ mean\r\nthat we cannot build useful systems while working within these\r\nconstraints.\r\n\r\nThe devil is in the details however. Before you start crying 'yes, but\r\nwhat about...', make sure you understand the following about exactly\r\nwhat the CAP theorem does and does not allow.\r\n\r\n## 3. What is 'read-write storage'?\r\n\r\nCAP specifically concerns itself with a theoretical\r\nconstruct called a _register_. A register is a data structure with two\r\noperations:\r\n\r\n* set(X) sets the value of the register to X\r\n* get() returns the last value set in the register\r\n\r\nA key-value store can be modelled as a collection of registers. Even\r\nthough registers appear very simple, they capture the essence of what\r\nmany distributed systems want to do - write data and read it back.\r\n\r\n## 4. What does _atomic_ (or _linearizable_) mean?\r\n\r\nAtomic, or linearizable, consistency is a guarantee about what values\r\nit's ok to return when a client performs get() operations. The idea is\r\nthat the register appears to all clients as though it ran on just one\r\nmachine, and responded to operations in the order they arrive.\r\n\r\nConsider an execution consisting the total set of operations performed\r\nby all clients, potentially concurrently. The results of those\r\noperations must, under atomic consistency, be equivalent to a single\r\nserial (in order, one after the other) execution of all operations.\r\n\r\nThis guarantee is very strong. It rules out, amongst other guarantees,\r\n_eventual consistency_, which allows a delay before a write becomes\r\nvisible. So under EC, you might have:\r\n\r\n    set(10), set(5), get() = 10\r\n\r\nBut this execution is invalid under atomic consistency.\r\n\r\nAtomic consistency also ensures that external communication about the\r\nvalue of a register is respected. That is, if I read X and tell you\r\nabout it, you can go to the register and read X for yourself. It's\r\npossible under slightly weaker guarantees (_serializability_ for\r\nexample) for that not to be true. In the following we write A:<set or\r\nget> to mean that client A executes the following operation.\r\n\r\n    B:set(5), A:set(10), A:get() = 10, B:get() = 10\r\n\r\nThis is an atomic history. But the following is not:\r\n\r\n    B:set(5), A:set(10), A:get() = 10, B:get() = 5\r\n\r\neven though it is equivalent to the following serial history:\r\n\r\n    B:set(5), B:get() = 5, A:set(10), B:get() = 10\r\n\r\nIn the second example, if A tells B about the value of the register\r\n(10) after it does its get(), B will falsely believe that some\r\nthird-party has written 5 between A:get() and B:get(). If external\r\ncommunication isn't allowed, B cannot know about A:set, and so sees a\r\nconsistent view of the register state; it's as if B:get really did\r\nhappen before A:set.\r\n\r\nWikipedia [1] has more information. Maurice Herlihy's original paper\r\nfrom 1990 is available at [2].\r\n\r\n[1] http://en.wikipedia.org/wiki/Linearizability\r\n[2] http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf\r\n\r\n## 4. What does _asynchronous_ mean?\r\n\r\nAn _asynchronous_ network is one in which there is no bound on how\r\nlong messages may take to be delivered by the network or processed by\r\na machine. The important consequence of this property is that there's\r\nno way to distinguish between a machine that has failed, and one whose\r\nmessages are getting delayed.\r\n\r\n## 5. What does _available_ mean?\r\n\r\nA data store is available if and only if all get and set requests\r\neventually return a response that's part of their specification. This\r\ndoes _not_ permit error responses, since a system could be trivially\r\navailable by always returning an error.\r\n\r\nThere is no requirement for a fixed time bound on the response, so the\r\nsystem can take as long as it likes to process a request. But the\r\nsystem must eventually respond.\r\n\r\nNotice how this is both a strong and a weak requirement. It's strong\r\nbecause 100% of the requests must return a response (there's no\r\n'degree of availability' here), but weak because the response can take\r\nan unbounded (but finite) amount of time.\r\n\r\n## 6. What is a _partition_?\r\n\r\nA partition is when the network fails to deliver some messages to one\r\nor more nodes by losing them (not by delaying them - eventual delivery\r\nis not a partition).\r\n\r\nThe term is sometimes used to refer to a period during which _no_\r\nmessages are delivered between two sets of nodes. This is a more\r\nrestrictive failure model. We'll call these kinds of partitions _total\r\npartitions_.\r\n\r\nThe proof of CAP relied on a total partition. In practice,\r\nthese are arguably the most likely since all messages may flow through\r\none component; if that fails then message loss is usually total\r\nbetween two nodes.\r\n\r\n## 7. Why is CAP true?\r\n\r\nThe basic idea is that if a client writes to one side of a partition,\r\nany reads that go to the other side of that partition can't possibly\r\nknow about the most recent write. Now you're faced with a choice: do\r\nyou respond to the reads with potentially stale information, or do you\r\nwait (potentially forever) to hear from the other side of the\r\npartition and compromise availability?\r\n\r\nThis is a proof by _construction_ - we demonstrate a single situation\r\nwhere a system cannot be consistent and available. One reason that CAP\r\ngets some press is that this constructed scenario is not completely\r\nunrealistic. It is not uncommon for a total partition to occur if\r\nnetworking equipment should fail.\r\n\r\n## 8. When does a system have to give up C or A?\r\n\r\nCAP only guarantees that there is _some_ circumstance in which a\r\nsystem must give up either C or A. Let's call that circumstance a\r\n_critical condition_.  The theorem doesn't say anything about how\r\nlikely that critical condition is. Both C and A are strong guarantees:\r\nthey hold only if 100% of operations meet their requirements. A single\r\ninconsistent read, or unavailable write, invalidates either C or\r\nA. But until that critical condition is met, a system can be happily\r\nconsistent _and_ available and not contravene any known laws.\r\n\r\nSince most distributed systems are long running, and may see millions\r\nof requests in their lifetime, CAP tells us to be\r\ncautious: there's a good chance that you'll realistically hit one of\r\nthese critical conditions, and it's prudent to understand how your\r\nsystem will fail to meet either C or A.\r\n\r\n## 9. Why do some people get annoyed when I characterise my system as CA?\r\n\r\nBrewer's keynote, the Gilbert paper, and many other treatments, places\r\nC, A and P on an equal footing as desirable properties of an\r\nimplementation and effectively say 'choose two!'. However, this is\r\noften considered to be a misleading presentation, since you cannot\r\nbuild - or choose! - 'partition tolerance': your system either might experience\r\npartitions or it won't.\r\n\r\nCAP is better understood as describing the tradeoffs you\r\nhave to make when you are building a system that may suffer\r\npartitions. In practice, this is every distributed system: there is no\r\n100% reliable network. So (at least in the distributed context) there\r\nis no realistic CA system. You will potentially suffer partitions,\r\ntherefore you must at some point compromise C or A.\r\n\r\nTherefore it's arguably more instructive to rewrite the theorem as the\r\nfollowing:\r\n\r\n<pre>Possibility of Partitions => Not (C and A)</pre>\r\n\r\ni.e. if your system may experience partitions, you can not always be C\r\nand A.\r\n\r\nThere are some systems that won't experience partitions - single-site\r\ndatabases, for example. These systems aren't generally relevant to the\r\ncontexts in which CAP is most useful. If you describe your distributed\r\ndatabase as 'CA', you are misunderstanding something.\r\n\r\n## 10. What about when messages don't get lost?\r\n\r\nA perhaps surprising result from the Gilbert paper is that no\r\nimplementation of an atomic register in an asynchronous network can be\r\navailable at all times, and consistent only when no messages are lost.\r\n\r\nThis result depends upon the asynchronous network property, the idea\r\nbeing that it is impossible to tell if a message has been dropped and\r\ntherefore a node cannot wait indefinitely for a response while still\r\nmaintaining availability, however if it responds too early it might be\r\ninconsistent.\r\n\r\n## 11. Is my network really asynchronous?\r\n\r\nArguably, yes. Different networks have vastly differing characteristics.\r\n\r\nIf\r\n\r\n* Your nodes do not have clocks (unlikely) or they have clocks that may drift apart (more likely)\r\n* System processes may arbitrarily delay delivery of a message (due to retries, or GC pauses)\r\n\r\nthen your network may be considered _asynchronous_.\r\n\r\nGilbert and Lynch also proved that in a _partially-synchronous_\r\nsystem, where nodes have shared but not synchronised clocks and there\r\nis a bound on the processing time of every message, that it is still\r\nimpossible to implement available atomic storage.\r\n\r\nHowever, the result from #8 does _not_ hold in the\r\npartially-synchronous model; it is possible to implement atomic\r\nstorage that is available all the time, and consistent when all\r\nmessages are delivered.\r\n\r\n## 12. What, if any, is the relationship between FLP and CAP?\r\n\r\nThe Fischer, Lynch and Patterson theorem ('FLP') (see [1] for a link\r\nto the paper and a proof explanation) is an extraordinary\r\nimpossibility result from nearly thirty years ago, which determined\r\nthat the problem of consensus - having all nodes agree on a common\r\nvalue - is unsolvable in general in asynchronous networks where one\r\nnode might fail.\r\n\r\nThe FLP result is not directly related to CAP, although they are\r\nsimilar in some respects. Both are impossibility results about\r\nproblems that may not be solved in distributed systems. The devil is\r\nin the details. Here are some of the ways in which FLP is different\r\nfrom CAP:\r\n\r\n* FLP permits the possibility of one 'failed' node which is totally\r\n  partitioned from the network and does not have to respond to\r\n  requests.\r\n* Otherwise, FLP does not allow message loss; the network\r\n  is only asynchronous but not lossy.\r\n* FLP deals with _consensus_, which is a similar but different problem\r\n  to _atomic storage_.\r\n\r\nFor a bit more on this topic, consult the blog post at [2].\r\n\r\n[1] http://the-paper-trail.org/blog/a-brief-tour-of-flp-impossibility/\r\n[2] http://the-paper-trail.org/blog/flp-and-cap-arent-the-same-thing/\r\n\r\n## 13. Are C and A 'spectrums'?\r\n\r\nIt is possible to relax both consistency and availability guarantees\r\nfrom the strong requirements that CAP imposes and get useful\r\nsystems. In fact, the whole point of CAP is that you\r\n_must_ do this, and any system you have designed and built relaxes one\r\nor both of these guarantees. The onus is on you to figure out when,\r\nand how, this occurs.\r\n\r\nReal systems choose to relax availability - in the case of systems for\r\nwhom consistency is of the utmost importance, like ZooKeeper. Other\r\nsystems, like Amazon's Dynamo, relax consistency in order to maintain\r\nhigh degrees of availability.\r\n\r\nOnce you weaken any of the assumptions made in the statement or proof\r\nof CAP, you have to start again when it comes to proving an\r\nimpossibility result.\r\n\r\n## 14. Is a failed machine the same as a partitioned one?\r\n\r\nNo. A 'failed' machine is usually excused the burden of having to\r\nrespond to client requests. CAP does not allow any machines to fail\r\n(in that sense it is a strong result, since it shows impossibility\r\nwithout having any machines fail).\r\n\r\nIt is possible to prove a similar result about the impossibility of\r\natomic storage in an asynchronous network when there are up to N-1\r\nfailures. This result has ramifications about the tradeoff between how\r\nmany nodes you write to (which is a performance concern) and how fault\r\ntolerant you are (which is a reliability concern).\r\n\r\n## 15. Is a slow machine the same as a partitioned one?\r\n\r\nNo: messages eventually get delivered to a slow machine, but they\r\nnever get delivered to a totally partitioned one. However, slow\r\nmachines play a significant role in making it very hard to distinguish\r\nbetween lost messages (or failed machines) and a slow machine. This\r\ndifficulty is right at the heart of why CAP, FLP and other results are\r\ntrue.\r\n\r\n## 16. Have I 'got around' or 'beaten' the CAP theorem?\r\n\r\nNo. You might have designed a system that is not heavily affected by\r\nit. That's good.\r\n","google":"UA-2671099-3","note":"Don't delete this file! It's used internally to help with page regeneration."}